{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "By: Laura Moses \n",
    "\n",
    "--- \n",
    "\n",
    "## Definition\n",
    "**Logistic Regression** is a supervised learning classification algorithm used to predict the probability of a target variable $y$. The nature of target or dependent variable is dichotomous, meaning there are only two possible classes. Thus, the target is binary, and represents either `1`, a success/yes/win, or `0`, a failure/no/loss. \n",
    "\n",
    "Mathematically, a logistic regression model predicts $P(Y=1)$ as a function of $X$. It is one of the simplest machine learning algorithms that can be used for various classification problems that involve detection.\n",
    "\n",
    "---\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "* The target variable is binary, with the desired outcome represented by the factor level 1\n",
    "* There is no multi-collinearity present in the model\n",
    "* Model variables are meaningful\n",
    "* Large sample size \n",
    "\n",
    "---\n",
    "\n",
    "### Advantages \n",
    "\n",
    "* Simple algorithm\n",
    "* Useful for predicting \n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Adding independent variables to model can result in overfitting\n",
    "* R^2^ can have computation issues causing them to be artifically high or low, so must be mindful when interpreting goodness of fit\n",
    "\n",
    "---\n",
    "\n",
    "## Model\n",
    "\n",
    "In this notebook, we will be using the dataset [SAheart](https://web.stanford.edu/~hastie/ElemStatLearn//datasets/SAheart.data), available within this repository, to predict if a person has coronary heart disease (CHD) based on some attributes.\n",
    "\n",
    "The dataset comes from a retrospective sample of males in a heart-disease high-risk region of the Western Cape, South Africa. There are roughly two controls per case of CHD, thus this is our response $y$. These data are taken from a larger dataset, described in  *Rousseauw et al, 1983, South African Medical Journal*. \n",
    "\n",
    "The dataset includes the following columns: \n",
    "\n",
    "* `sbp` = Systolic blood pressure\n",
    "* `tobacco`\t= Cumulative tobacco use (kg)\n",
    "* `ldl` = Low densiity lipoprotein cholesterol\n",
    "* `adiposity`\n",
    "* `famhist` = Family history of heart disease (Present, Absent)\n",
    "* `typea` = Type-A behavior\n",
    "* `obesity`\n",
    "* `alcohol` = Current alcohol consumption\n",
    "* `age`\t= Age at onset\n",
    "* `chd` = *Target*, coronary heart disease (1, 0)\n",
    "\n",
    "---\n",
    "\n",
    "### Method\n",
    "\n",
    "We want to predict whether or not a male has coronary heart disease (`chd`) based on three features: `sbp`, `ldl`, and `alcohol`.\n",
    "\n",
    "**<ins>Feed Forward</ins>** (with a single neuron)\n",
    "$$\n",
    "(x^1, y^1),\\cdots,(x^N, y^N); \\space \\space\n",
    "x^i =  \n",
    " \\left[{\\begin{array}{c}\n",
    "male \\  i \\  sys \\  bp \\\\\n",
    "male \\  i \\  ldl \\  chol \\\\\n",
    "male \\  i \\  alcohol\n",
    " \\end{array}}\\right] \\\\\n",
    "y^i \\in \\{0,1\\} \\\\\n",
    "where \\  1 \\  indicates \\  male \\  i \\  had \\  CHD, \\  0 \\  otherwise\n",
    " $$\n",
    " \n",
    " $$Z^i = w^T \\cdot x^i + b$$\n",
    " \n",
    "---\n",
    "\n",
    "**<ins>Loss Function</ins>**\n",
    "\n",
    "We want $L(\\hat {y^i}, y^i)=$ How close $\\hat{y^i}$ is to $y^i$!\n",
    "\n",
    "First consider maximizing $P(y^i|x^i)$, the probability that $\\hat{y^i}$ predicts $y^i$. Since there are two discrete outputs, this is subject to the following formula by Bernoulli:\n",
    "\n",
    "Maximize $\\rightarrow P(y^i|x^i) = \\hat y^y (1-\\hat y)^{1-y}$\n",
    "\n",
    "**<ins>Decision Boundary</ins>**\n",
    "Prediction is $1$ if $\\hat {y^i} \\ge 0.5$, $0$ otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "The following packages will be needed to run the code below: \n",
    "\n",
    "* CSV [documentation](https://csv.juliadata.org/stable/)\n",
    "* DataFrames [documentation](https://dataframes.juliadata.org/stable/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>row.names</th><th>sbp</th><th>tobacco</th><th>ldl</th><th>adiposity</th><th>famhist</th><th>typea</th><th>obesity</th><th>alcohol</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>String</th><th>Int64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>462 rows Ã— 11 columns (omitted printing of 2 columns)</p><tr><th>1</th><td>1</td><td>160</td><td>12.0</td><td>5.73</td><td>23.11</td><td>Present</td><td>49</td><td>25.3</td><td>97.2</td></tr><tr><th>2</th><td>2</td><td>144</td><td>0.01</td><td>4.41</td><td>28.61</td><td>Absent</td><td>55</td><td>28.87</td><td>2.06</td></tr><tr><th>3</th><td>3</td><td>118</td><td>0.08</td><td>3.48</td><td>32.28</td><td>Present</td><td>52</td><td>29.14</td><td>3.81</td></tr><tr><th>4</th><td>4</td><td>170</td><td>7.5</td><td>6.41</td><td>38.03</td><td>Present</td><td>51</td><td>31.99</td><td>24.26</td></tr><tr><th>5</th><td>5</td><td>134</td><td>13.6</td><td>3.5</td><td>27.78</td><td>Present</td><td>60</td><td>25.99</td><td>57.34</td></tr><tr><th>6</th><td>6</td><td>132</td><td>6.2</td><td>6.47</td><td>36.21</td><td>Present</td><td>62</td><td>30.77</td><td>14.14</td></tr><tr><th>7</th><td>7</td><td>142</td><td>4.05</td><td>3.38</td><td>16.2</td><td>Absent</td><td>59</td><td>20.81</td><td>2.62</td></tr><tr><th>8</th><td>8</td><td>114</td><td>4.08</td><td>4.59</td><td>14.6</td><td>Present</td><td>62</td><td>23.11</td><td>6.72</td></tr><tr><th>9</th><td>9</td><td>114</td><td>0.0</td><td>3.83</td><td>19.4</td><td>Present</td><td>49</td><td>24.86</td><td>2.49</td></tr><tr><th>10</th><td>10</td><td>132</td><td>0.0</td><td>5.8</td><td>30.96</td><td>Present</td><td>69</td><td>30.11</td><td>0.0</td></tr><tr><th>11</th><td>11</td><td>206</td><td>6.0</td><td>2.95</td><td>32.27</td><td>Absent</td><td>72</td><td>26.81</td><td>56.06</td></tr><tr><th>12</th><td>12</td><td>134</td><td>14.1</td><td>4.44</td><td>22.39</td><td>Present</td><td>65</td><td>23.09</td><td>0.0</td></tr><tr><th>13</th><td>13</td><td>118</td><td>0.0</td><td>1.88</td><td>10.05</td><td>Absent</td><td>59</td><td>21.57</td><td>0.0</td></tr><tr><th>14</th><td>14</td><td>132</td><td>0.0</td><td>1.87</td><td>17.21</td><td>Absent</td><td>49</td><td>23.63</td><td>0.97</td></tr><tr><th>15</th><td>15</td><td>112</td><td>9.65</td><td>2.29</td><td>17.2</td><td>Present</td><td>54</td><td>23.53</td><td>0.68</td></tr><tr><th>16</th><td>16</td><td>117</td><td>1.53</td><td>2.44</td><td>28.95</td><td>Present</td><td>35</td><td>25.89</td><td>30.03</td></tr><tr><th>17</th><td>17</td><td>120</td><td>7.5</td><td>15.33</td><td>22.0</td><td>Absent</td><td>60</td><td>25.31</td><td>34.49</td></tr><tr><th>18</th><td>18</td><td>146</td><td>10.5</td><td>8.29</td><td>35.36</td><td>Present</td><td>78</td><td>32.73</td><td>13.89</td></tr><tr><th>19</th><td>19</td><td>158</td><td>2.6</td><td>7.46</td><td>34.07</td><td>Present</td><td>61</td><td>29.3</td><td>53.28</td></tr><tr><th>20</th><td>20</td><td>124</td><td>14.0</td><td>6.23</td><td>35.96</td><td>Present</td><td>45</td><td>30.09</td><td>0.0</td></tr><tr><th>21</th><td>21</td><td>106</td><td>1.61</td><td>1.74</td><td>12.32</td><td>Absent</td><td>74</td><td>20.92</td><td>13.37</td></tr><tr><th>22</th><td>22</td><td>132</td><td>7.9</td><td>2.85</td><td>26.5</td><td>Present</td><td>51</td><td>26.16</td><td>25.71</td></tr><tr><th>23</th><td>23</td><td>150</td><td>0.3</td><td>6.38</td><td>33.99</td><td>Present</td><td>62</td><td>24.64</td><td>0.0</td></tr><tr><th>24</th><td>24</td><td>138</td><td>0.6</td><td>3.81</td><td>28.66</td><td>Absent</td><td>54</td><td>28.7</td><td>1.46</td></tr><tr><th>25</th><td>25</td><td>142</td><td>18.2</td><td>4.34</td><td>24.38</td><td>Absent</td><td>61</td><td>26.19</td><td>0.0</td></tr><tr><th>26</th><td>26</td><td>124</td><td>4.0</td><td>12.42</td><td>31.29</td><td>Present</td><td>54</td><td>23.23</td><td>2.06</td></tr><tr><th>27</th><td>27</td><td>118</td><td>6.0</td><td>9.65</td><td>33.91</td><td>Absent</td><td>60</td><td>38.8</td><td>0.0</td></tr><tr><th>28</th><td>28</td><td>145</td><td>9.1</td><td>5.24</td><td>27.55</td><td>Absent</td><td>59</td><td>20.96</td><td>21.6</td></tr><tr><th>29</th><td>29</td><td>144</td><td>4.09</td><td>5.55</td><td>31.4</td><td>Present</td><td>60</td><td>29.43</td><td>5.55</td></tr><tr><th>30</th><td>30</td><td>146</td><td>0.0</td><td>6.62</td><td>25.69</td><td>Absent</td><td>60</td><td>28.07</td><td>8.23</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& row.names & sbp & tobacco & ldl & adiposity & famhist & typea & obesity & alcohol & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Float64 & Float64 & Float64 & String & Int64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 160 & 12.0 & 5.73 & 23.11 & Present & 49 & 25.3 & 97.2 & $\\dots$ \\\\\n",
       "\t2 & 2 & 144 & 0.01 & 4.41 & 28.61 & Absent & 55 & 28.87 & 2.06 & $\\dots$ \\\\\n",
       "\t3 & 3 & 118 & 0.08 & 3.48 & 32.28 & Present & 52 & 29.14 & 3.81 & $\\dots$ \\\\\n",
       "\t4 & 4 & 170 & 7.5 & 6.41 & 38.03 & Present & 51 & 31.99 & 24.26 & $\\dots$ \\\\\n",
       "\t5 & 5 & 134 & 13.6 & 3.5 & 27.78 & Present & 60 & 25.99 & 57.34 & $\\dots$ \\\\\n",
       "\t6 & 6 & 132 & 6.2 & 6.47 & 36.21 & Present & 62 & 30.77 & 14.14 & $\\dots$ \\\\\n",
       "\t7 & 7 & 142 & 4.05 & 3.38 & 16.2 & Absent & 59 & 20.81 & 2.62 & $\\dots$ \\\\\n",
       "\t8 & 8 & 114 & 4.08 & 4.59 & 14.6 & Present & 62 & 23.11 & 6.72 & $\\dots$ \\\\\n",
       "\t9 & 9 & 114 & 0.0 & 3.83 & 19.4 & Present & 49 & 24.86 & 2.49 & $\\dots$ \\\\\n",
       "\t10 & 10 & 132 & 0.0 & 5.8 & 30.96 & Present & 69 & 30.11 & 0.0 & $\\dots$ \\\\\n",
       "\t11 & 11 & 206 & 6.0 & 2.95 & 32.27 & Absent & 72 & 26.81 & 56.06 & $\\dots$ \\\\\n",
       "\t12 & 12 & 134 & 14.1 & 4.44 & 22.39 & Present & 65 & 23.09 & 0.0 & $\\dots$ \\\\\n",
       "\t13 & 13 & 118 & 0.0 & 1.88 & 10.05 & Absent & 59 & 21.57 & 0.0 & $\\dots$ \\\\\n",
       "\t14 & 14 & 132 & 0.0 & 1.87 & 17.21 & Absent & 49 & 23.63 & 0.97 & $\\dots$ \\\\\n",
       "\t15 & 15 & 112 & 9.65 & 2.29 & 17.2 & Present & 54 & 23.53 & 0.68 & $\\dots$ \\\\\n",
       "\t16 & 16 & 117 & 1.53 & 2.44 & 28.95 & Present & 35 & 25.89 & 30.03 & $\\dots$ \\\\\n",
       "\t17 & 17 & 120 & 7.5 & 15.33 & 22.0 & Absent & 60 & 25.31 & 34.49 & $\\dots$ \\\\\n",
       "\t18 & 18 & 146 & 10.5 & 8.29 & 35.36 & Present & 78 & 32.73 & 13.89 & $\\dots$ \\\\\n",
       "\t19 & 19 & 158 & 2.6 & 7.46 & 34.07 & Present & 61 & 29.3 & 53.28 & $\\dots$ \\\\\n",
       "\t20 & 20 & 124 & 14.0 & 6.23 & 35.96 & Present & 45 & 30.09 & 0.0 & $\\dots$ \\\\\n",
       "\t21 & 21 & 106 & 1.61 & 1.74 & 12.32 & Absent & 74 & 20.92 & 13.37 & $\\dots$ \\\\\n",
       "\t22 & 22 & 132 & 7.9 & 2.85 & 26.5 & Present & 51 & 26.16 & 25.71 & $\\dots$ \\\\\n",
       "\t23 & 23 & 150 & 0.3 & 6.38 & 33.99 & Present & 62 & 24.64 & 0.0 & $\\dots$ \\\\\n",
       "\t24 & 24 & 138 & 0.6 & 3.81 & 28.66 & Absent & 54 & 28.7 & 1.46 & $\\dots$ \\\\\n",
       "\t25 & 25 & 142 & 18.2 & 4.34 & 24.38 & Absent & 61 & 26.19 & 0.0 & $\\dots$ \\\\\n",
       "\t26 & 26 & 124 & 4.0 & 12.42 & 31.29 & Present & 54 & 23.23 & 2.06 & $\\dots$ \\\\\n",
       "\t27 & 27 & 118 & 6.0 & 9.65 & 33.91 & Absent & 60 & 38.8 & 0.0 & $\\dots$ \\\\\n",
       "\t28 & 28 & 145 & 9.1 & 5.24 & 27.55 & Absent & 59 & 20.96 & 21.6 & $\\dots$ \\\\\n",
       "\t29 & 29 & 144 & 4.09 & 5.55 & 31.4 & Present & 60 & 29.43 & 5.55 & $\\dots$ \\\\\n",
       "\t30 & 30 & 146 & 0.0 & 6.62 & 25.69 & Absent & 60 & 28.07 & 8.23 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m462Ã—11 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0mâ”‚\u001b[1m row.names \u001b[0m\u001b[1m sbp   \u001b[0m\u001b[1m tobacco \u001b[0m\u001b[1m ldl     \u001b[0m\u001b[1m adiposity \u001b[0m\u001b[1m famhist \u001b[0m\u001b[1m typea \u001b[0m\u001b[1m obesity \u001b[0m\u001b[1m\u001b[0m â‹¯\n",
       "\u001b[1m     \u001b[0mâ”‚\u001b[90m Int64     \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m String  \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m â‹¯\n",
       "â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "   1 â”‚         1    160    12.0      5.73      23.11  Present     49    25.3   â‹¯\n",
       "   2 â”‚         2    144     0.01     4.41      28.61  Absent      55    28.87\n",
       "   3 â”‚         3    118     0.08     3.48      32.28  Present     52    29.14\n",
       "   4 â”‚         4    170     7.5      6.41      38.03  Present     51    31.99\n",
       "   5 â”‚         5    134    13.6      3.5       27.78  Present     60    25.99  â‹¯\n",
       "   6 â”‚         6    132     6.2      6.47      36.21  Present     62    30.77\n",
       "   7 â”‚         7    142     4.05     3.38      16.2   Absent      59    20.81\n",
       "   8 â”‚         8    114     4.08     4.59      14.6   Present     62    23.11\n",
       "   9 â”‚         9    114     0.0      3.83      19.4   Present     49    24.86  â‹¯\n",
       "  10 â”‚        10    132     0.0      5.8       30.96  Present     69    30.11\n",
       "  11 â”‚        11    206     6.0      2.95      32.27  Absent      72    26.81\n",
       "  â‹®  â”‚     â‹®        â‹®       â‹®        â‹®         â‹®         â‹®       â‹®       â‹®     â‹±\n",
       " 453 â”‚       454    154     5.53     3.2       28.81  Present     61    26.15\n",
       " 454 â”‚       455    124     1.6      7.22      39.68  Present     36    31.5   â‹¯\n",
       " 455 â”‚       456    146     0.64     4.82      28.02  Absent      60    28.11\n",
       " 456 â”‚       457    128     2.24     2.83      26.48  Absent      48    23.96\n",
       " 457 â”‚       458    170     0.4      4.11      42.06  Present     56    33.1\n",
       " 458 â”‚       459    214     0.4      5.98      31.72  Absent      64    28.45  â‹¯\n",
       " 459 â”‚       460    182     4.2      4.41      32.1   Absent      52    28.61\n",
       " 460 â”‚       461    108     3.0      1.59      15.23  Absent      40    20.09\n",
       " 461 â”‚       462    118     5.4     11.61      30.79  Absent      64    27.35\n",
       " 462 â”‚       463    132     0.0      4.82      33.41  Present     62    14.7   â‹¯\n",
       "\u001b[36m                                                  3 columns and 441 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "\n",
    "data = CSV.read(\"SAheart.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[x[1], x[2], x[3]] for x in zip(data.sbp, data.ldl, data.alcohol)]\n",
    "y_data = [x for x in data.chd];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Below we will define a sigmoid activation function to map predictions to probabilities using the following: \n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z^i}}$$\n",
    "\n",
    "where $\\sigma(z)$ is an output between $0$ and $1$ (probability estimate), $z^i$ is the algorithms prediction ($wx^i + b$), and $e$ is Euler's number. \n",
    "\n",
    "We will use stochastic gradient descent to update our weights and bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "Ïƒ(x) = 1/(1+exp(-x))\n",
    "\n",
    "# Loss function\n",
    "function cross_entropy_loss(x, y, w, b)\n",
    "    return -y*log(Ïƒ(w'x + b)) - (1-y)*log(1 - Ïƒ(w'x+b))\n",
    "end\n",
    "\n",
    "# Cost function\n",
    "function average_cost(features, labels, w, b)\n",
    "    N = length(features)\n",
    "    return (1/N)*sum([cross_entropy_loss(features[i], labels[i], w, b) for i = 1:N])\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE AND REPLACE WITH STOCHASTIC?\n",
    "function batch_gradient_descent(features, labels, w, b, Î±)\n",
    "    del_w = [0.0 for i = 1:length(w)]\n",
    "    del_b = 0.0\n",
    "    \n",
    "    N = length(features)\n",
    "    \n",
    "    for i = 1:N\n",
    "        del_w += (Ïƒ(w'features[i] + b) - labels[i]) * features[i]\n",
    "        del_b += (Ïƒ(w'features[i] + b) - labels[i])\n",
    "    end\n",
    "    \n",
    "    w = w - Î±*del_w\n",
    "    b = b - Î±*del_b\n",
    "    \n",
    "    return w, b\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now we will test our functions to make sure the cost is decreasing with each iteration.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial cost is: 0.6931471805599451\n",
      "The new cost is: 0.6931296464273247\n",
      "The new cost is: 0.6931121203412501\n",
      "The new cost is: 0.6930946022980257\n",
      "The new cost is: 0.6930770922939568\n"
     ]
    }
   ],
   "source": [
    "#### CHANGE TO STOACHISTIC\n",
    "# Initialize weights and bias, choose alpha\n",
    "w = [0.0, 0.0, 0.0]\n",
    "b = 0.0\n",
    "alpha = 0.0000000001\n",
    "\n",
    "println(\"The initial cost is: \", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "for i in 1:4\n",
    "    w, b = batch_gradient_descent(x_data, y_data, w, b, alpha)\n",
    "    println(\"The new cost is: \", average_cost(x_data, y_data, w, b))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we've verified our stochastic gradient descent function is working, we can define a training stochastic gradient descent function below: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_batch_gradient_descent(features, labels, w, b, Î±, epochs)\n",
    "    for i = 1:epochs\n",
    "        w, b = batch_gradient_descent(features, labels, w, b, Î±)\n",
    "        \n",
    "        if i == 1\n",
    "            println(\"Epoch \", i, \" with cost: \", average_cost(features, labels, w, b))\n",
    "        end\n",
    "        \n",
    "        if i == 100\n",
    "            println(\"Epoch \", i, \" with cost: \", average_cost(features, labels, w, b))\n",
    "        end\n",
    "        \n",
    "        if i == 1000\n",
    "            println(\"Epoch \", i, \" with cost: \", average_cost(features, labels, w, b))\n",
    "        end\n",
    "        \n",
    "        if i == 10000\n",
    "            println(\"Epoch \", i, \" with cost: \", average_cost(features, labels, w, b))\n",
    "        end        \n",
    "        \n",
    "        if i == 100000\n",
    "            println(\"Epoch \", i, \" with cost: \", average_cost(features, labels, w, b))\n",
    "        end\n",
    "    end\n",
    "    return w, b\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DELETE -- Modified to not print output\n",
    "\n",
    "function train_batch_gradient_descent(features, labels, w, b, Î±, epochs)\n",
    "    for i = 1:epochs\n",
    "        w, b = batch_gradient_descent(features, labels, w, b, Î±)\n",
    "    end\n",
    "    println(\"Cost: \", average_cost(features, labels, w, b))\n",
    "    return w, b\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "function stochastic_gradient_descent(x_data, y_data, w, b, Î±)\n",
    "    \n",
    "    N = length(x_data)\n",
    "    \n",
    "    # pick a random i\n",
    "    i = rand([k for k = 1:N])\n",
    "    \n",
    "    # calculate the gradient only at randomly picked point\n",
    "    w = w - (-2/N)*Î±*x_data[i]*(y_data[i] - (w*x_data[i] + b))\n",
    "    b = b - (-2/N)*Î±*(y_data[i] - (w*x_data[i] + b))\n",
    "    \n",
    "    return w, b\n",
    "end\n",
    "\n",
    "function stochastic_train(x_data, y_data, w, b, Î±, epochs)\n",
    "    for i = 1:epochs\n",
    "        w, b = batch_gradient_descent(x_data, y_data, w, b, Î±)\n",
    "    end\n",
    "    println(average_cost(x_data, y_data, w, b))\n",
    "    return w, b\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with cost: 0.6483759045108881\n",
      "Epoch 100 with cost: 0.648363893476474\n",
      "Epoch 1000 with cost: 0.6482748653761067\n",
      "Epoch 10000 with cost: 0.6480785925985565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.00552190290970136, 0.029073227126353927, 0.003927708039620779], -0.00021305501678453012)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### EDIT THIS TO HAVE STOCHASTIC\n",
    "# Initializing 0 for all weights and bias\n",
    "w = [0.0, 0.0, 0.0]\n",
    "b = 0.0\n",
    "alpha = 0.0000000001\n",
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, w, b, alpha, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "While training our $w$ and $b$, the goal is to get our cost as close to $0.5$ as possible, since this is our decision boundary. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with cost: 0.6469123174328647\n",
      "Epoch 100 with cost: 0.6469122075870735\n",
      "Epoch 1000 with cost: 0.6469112090345196\n",
      "Epoch 10000 with cost: 0.646901228021375\n",
      "Epoch 100000 with cost: 0.6468018677642113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.00604427938642357, 0.04330469167940441, 0.003947580425113944], -0.0017567425450463032)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = train_batch_gradient_descent(x_data, y_data, w, b, 0000000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with cost: 0.6458519141416657\n",
      "Epoch 100 with cost: 0.6458518138824914\n",
      "Epoch 1000 with cost: 0.6458509024770221\n",
      "Epoch 10000 with cost: 0.6458417925404434\n",
      "Epoch 100000 with cost: 0.6457511037292287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.006271313325661864, 0.04994181753659419, 0.003985229555625332], -0.002533610719213252)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = train_batch_gradient_descent(x_data, y_data, w, b, alpha, 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can see that our cost was decreasing each time we ran the training batch descent. Now, we continue to re-run the code cell below until we reach approximately $0.50$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with cost: 0.644878495686086\n",
      "Epoch 100 with cost: 0.6448784955946222\n",
      "Epoch 1000 with cost: 0.6448784947631326\n",
      "Epoch 10000 with cost: 0.644878486448249\n",
      "Epoch 100000 with cost: 0.6448784032997599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.006272865982679547, 0.04998720227642257, 0.003985487896146975], -0.002539060225144372)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = train_batch_gradient_descent(x_data, y_data, w, b, alpha, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6448701842088637\n",
      "0.6448692611324501\n",
      "0.6448683381402383\n"
     ]
    }
   ],
   "source": [
    "while average_cost(x_data, y_data, w, b) >= .60\n",
    "    w, b = stochastic_train(x_data, y_data, w, b, alpha, epochs)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(x, y, w, b)\n",
    "    if Ïƒ(w'x+b) >= .5\n",
    "        println(\"Predict Accepted\")\n",
    "        y == 1 ? println(\"Was Accepted\") : println(\"Was Not Accepted\")\n",
    "    else\n",
    "        println(\"Predict Not Accepted\")\n",
    "        y == 1 ? println(\"Was Accepted\") : println(\"Was Not Accepted\")\n",
    "        \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i = 1:length(x_data)\n",
    "    predict(x_data[i], y_data[i], w, b)\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function new_predict(x, y, w, b)\n",
    "    if Ïƒ(w'x+b) >= .5\n",
    "        return 1\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error = 0.0\n",
    "for i = 1:length(x_data)\n",
    "    mean_error += (new_predict(x_data[i], y_data[i], w, b) - y_data[i])^2\n",
    "end\n",
    "\n",
    "println(mean_error/length(x_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
